---
layout: home
author_profile: true
---
<head>
	<style>
		.project{
		  width: 100%;
		  margin: 50px auto;
		  overflow: hidden;
		}
		.teaser{
		  width:20%;
		  float:left;
		  padding: 0px;
		}
		.teaser img{
		  display: inline-block;
		  text-align-last: center;
/*		  width:200%;*/
		}
		.intro{
		  float: right;
		  width: 80%;
		  padding: 0px 0px 0px 20px;
		}
	</style>
</head>

<body>

<h1> Kunpeng Wang </h1>

<p style="text-align:justify"><small>
	Hello! I am a Third-year Ph.D. student in School of Computer Science and Technology at Anhui University (AHU), advised by Prof. <a href="https://cs.ahu.edu.cn/2022/0307/c20806a280841/page.htm">Bin Luo</a> and Prof. <a href="https://chenglongli.cn/">Chenglong Li</a>. In 2025, I was sponsored by the China Scholarship Council (CSC) to study as a visiting student in the University of Technology Sydney (UTS), Faculty of Engineering and Information Technology, Australia, advised by Prof. <a href="https://profiles.uts.edu.au/XiaoJun.Chang">Chang Xiaojun</a>. My research interests lie in the fields of computer vision, deep learning, and pattern recognition.
</small></p>

<p><small>
	During my PhD study, I am supported by the Youth Talents Support Project - Doctoral Student Special Program (首届中国科协青年人才托举工程-博士生专项, <a href="https://cs.ahu.edu.cn/2025/0108/c11156a355939/page.htm">News</a>). I am also a reviewer for international conferences and journals, such as ACM MM (2024), IEEE TRANSACTIONS ON IMAGE PROCESSING, ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,  Scientific Reports.
</small></p>

<p><small>
	Currently, I am actively seeking full-time research opportunities, beginning in Winter 2025. If you're aware of any suitable positions, I would greatly appreciate it if you could reach out to me.
</small></p>


<!-- <h2> Research Interests </h2>
<hr />
<small><p>
Over the past few years, I have delved in several fascinating research topics, including generative models, adversarial examiners, and 3D computer vision. My journey began with robotic manipulation using <a href="https://arxiv.org/abs/1811.10264">reinforcement learning</a> and <a href="https://arxiv.org/abs/2012.00088">3D computer vision</a>. Recognizing robustness as a critical challenge for real-world applications of computer vision and other AI systems, I turned my focus to integrating <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">human knowledge</a>, <a href="https://arxiv.org/abs/2303.07337">graph-based models</a>, and <a href="https://arxiv.org/abs/2306.08103">generative models</a> to enhance both performance and robustness of vision systems. These efforts led to the development of <a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec25/AdversarialExaminerIntro.pdf">adversarial examiner</a>, which I further applied to generative models to systematically uncover and analyze their <a href="https://arxiv.org/abs/2306.00974">failure modes</a>.
</p></small>
<small><p>
Recognizing that generative and discriminative models can enhance each other—through methods including adversarial examiners—and the relatively underexplored potential of generative models, I also focused on improving generative models for image, 3D, and video generation. For example, <a href="https://arxiv.org/abs/2406.09416">DiMR</a> progressively refines details from low to high resolution, significantly reducing distortions and enhancing visual fidelity in image generation. <a href="https://arxiv.org/pdf/2412.15213">CrossFlow</a> introduces a novel approach for T2I generation by directly mapping between modalities with flow matching, bypassing the traditional reliance on Gaussian noise. <a href="https://arxiv.org/abs/2406.04322">DIRECT-3D</a> establishes a new training paradigm for 3D generative models, enabling effective learning from large-scale, noisy, and unaligned 3D data sourced from the Internet. ReVision utilizes explicit 3D knowledge to improve video generation, particularly for generating complex motions.
</p></small>
<small><p>
I have also worked on topics including segmentation, video understanding, foundational object models, and 3D datasets during my internships and collaborations.
</p></small>
<small><p>
In short, during my PhD studies, I have been the core contributor to several projects that align closely with my research interests:
</p></small> -->
<!-- <small><p>My research interests lie in the fields of 3D computer vision, generative models, robustness, and robotics. Specifically, my recent work focuses on:</p></small> -->
<!-- <ul>
	<li style="margin:0px"><small>Image/3D/video generation [<a href="https://arxiv.org/abs/2306.08103">ICLR24</a>, <a href="https://arxiv.org/abs/2406.04322">CVPR24</a>, <a href="https://arxiv.org/abs/2406.09416">NeurIPS24</a>, <a href="https://arxiv.org/pdf/2412.15213">Arxiv24</a>]</small></li>
	<li style="margin:0px"><small>Evaluating and improving ML system with adversarial examiner [<a href="https://arxiv.org/abs/2303.07337">CVPR23</a>, <a href="https://arxiv.org/abs/2306.00974">ICLR24</a>]</small></li>
	<li style="margin:0px"><small>Video instance segmentation and tracking [<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19815-1_34.pdf">ECCV22</a>, <a href="https://arxiv.org/abs/2303.08132">CVPR23</a>]</small></li>
	<li style="margin:0px"><small>3D human/articulated object pose estimation [<a href="https://arxiv.org/abs/2012.00088">Arxiv20</a>, <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">ECCV22</a>]</small></li>
	<li style="margin:0px"><small>Reinforcement learning for robot manipulation [<a href="https://arxiv.org/abs/1811.10264">IROS21</a>]</small></li>
</ul> -->


<!-- <h2> News </h2>
<hr />
<ul style="margin:0px auto">
	<li style="margin:0px auto"><small>[2024.12] <a href="https://cross-flow.github.io/">CrossFlow</a> is released! Code and models are available <a href="https://github.com/qihao067/CrossFlow">here</a></small></small>.</li>
	<li style="margin:0px auto"><small>[2024.09] <a href="https://imagenet3d.github.io">ImageNet3D</a> is accepted at NeurIPS D&B Track 2024.</small></li>
	<li style="margin:0px auto"><small>[2024.09] <a href="https://qihao067.github.io/projects/DiMR">DiMR</a> is accepted at NeurIPS 2024. Code is available <a href="https://github.com/qihao067/DiMR">here</a></small>.</li>
	<li style="margin:0px auto"><small>[2024.02] <a href="https://direct-3d.github.io/">DIRECT-3D</a> and <a href="https://glee-vision.github.io/">GLEE</a> are accepted at CVPR 2024, with code and models available <a href="https://github.com/qihao067/direct3d">here</a> and <a href="https://github.com/FoundationVision/GLEE">here</a></small>.</li>
	<li style="margin:0px auto"><small>[2024.01] <a href="https://sage-diffusion.github.io/">SAGE</a> and <a href="https://ccvl.jhu.edu/3D-DST/">3D-DST</a> are accepted at ICLR 2024, with 3D-DST honored as a spotlight presentation.</small></li>
	<li style="margin:0px auto"><small>[2023.07] <a href="https://xujiacong.github.io/Animal3D/">Animal3D</a> is accepted at ICCV 2023.</small></li>
	<li style="margin:0px auto"><small>[2023.02] <a href="https://arxiv.org/abs/2303.07337">PoseExaminer</a> and <a href="https://arxiv.org/abs/2303.08132">InstMove</a> are accepted at CVPR 2023.</small></li>
	<li style="margin:0px auto"><small>[2022.07] <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19815-1_34.pdf">IDOL</a> and <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">HUPOR</a> are accepted at ECCV 2022, including one oral.</small></li>
	<li style="margin:0px auto"><small>[2022.03] <a href="https://qliu24.github.io/udapart/">UDA-Part</a> is accepted at CVPR 2022 for oral presentation.</small></li>
	<li style="margin:0px auto"><small>[2021.07] <a href="https://ieeexplore.ieee.org/abstract/document/9636234/">PNS</a> is accepted at IROS 2021.</small></li>
	<li style="margin:0px auto"><small>[2021.04] I will join the Computational Cognition, Vision, and Learning (CCVL) Lab as a Ph.D. student at Johns Hopkins University in August 2021.</small></li>
</ul> -->

<h2> Publications </h2>
<hr />
<!-- <b> Selected Papers: </b> -->


<!-- Sammese -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/Sammese.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and Progressive Correlation Network </b></font><br />
		<small> <b>Kunpeng Wang</b>, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo </small><br />
		<small> Submitted to SCIS 2025 </small><br />
		<!-- <small> [TL;DR] CrossFlow is a simple, general framework that maps between two modalities using standard flow matching without additional conditioning, achieving SOTA results across tasks like T2I, depth estimation, and image captioning—without cross-attention or task-specific designs.  </small><br /> -->
		<small>( <a href="https://arxiv.org/pdf/2408.15063">Paper</a> | <a href="https://github.com/Angknpng/Sammese">Code</a> ) </small>
	</div>
</div>



<!-- UniSOD -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/UniSOD.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Unified-modal Salient Object Detection via Adaptive Prompt Learning </b></font><br />
		<small> <b>Kunpeng Wang</b>, Chenglong Li, Zhengzheng Tu, Zhengyi Liu, Bin Luo </small><br />
		<small> Submitted to TNNLS 2025 </small><br />
		<!-- <small> [TL;DR] CrossFlow is a simple, general framework that maps between two modalities using standard flow matching without additional conditioning, achieving SOTA results across tasks like T2I, depth estimation, and image captioning—without cross-attention or task-specific designs.  </small><br /> -->
		<small>( <a href="https://arxiv.org/pdf/2311.16835">Paper</a> | <a href="https://github.com/Angknpng/UniSOD">Code</a> ) </small>
	</div>
</div>



<!-- PCNet -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/PCNet.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and Progressive Correlation Network </b></font><br />
		<small> <b>Kunpeng Wang</b>, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo </small><br />
		<small> Association for the Advancement of Artificial Intelligence (AAAI), 2025 </small><br />
		<!-- <small> [TL;DR] CrossFlow is a simple, general framework that maps between two modalities using standard flow matching without additional conditioning, achieving SOTA results across tasks like T2I, depth estimation, and image captioning—without cross-attention or task-specific designs.  </small><br /> -->
		<small>( <a href="https://arxiv.org/pdf/2412.14576">Paper</a> | <a href="https://github.com/Angknpng/PCNet">Code</a> ) </small>
	</div>
</div>



<!-- SACNet -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/SACNet.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Alignment-Free RGBT Salient Object Detection: Semantics-guided Asymmetric Correlation Network and A Unified Benchmark </b></font><br />
		<small> <b>Kunpeng Wang</b>, Danying Lin, Chenglong Li, Zhengzheng Tu, Bin Luo </small><br />
		<small> IEEE Transactions on Multimedia (TMM), 2024 </small><br />
		<!-- <small> [TL;DR] DiMR is a new diffusion backbone that achieves state-of-the-art image generation. For example, on the ImageNet 256 x 256 benchmark, DiMR with only 505M parameters surpasses all existing image generation models of various sizes, without any bells and whistles.  </small><br /> -->
		<small>( <a href="https://arxiv.org/pdf/2406.00917">Paper</a> | <a href="https://github.com/Angknpng/SACNet">Code</a> ) </small>
	</div>
</div>



<!-- LAFB -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/LAFB.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Learning Adaptive Fusion Bank for Multi-modal Salient Object Detection </b></font><br />
		<small> <b>Kunpeng Wang</b>, Zhengzheng Tu, Chenglong Li, Cheng Zhang, Bin Luo </small><br />
		<small> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024 </small><br />
		<!-- <small> [TL;DR] DiMR is a new diffusion backbone that achieves state-of-the-art image generation. For example, on the ImageNet 256 x 256 benchmark, DiMR with only 505M parameters surpasses all existing image generation models of various sizes, without any bells and whistles.  </small><br /> -->
		<small>( <a href="https://arxiv.org/pdf/2406.01127">Paper</a> | <a href="https://github.com/Angknpng/LAFB">Code</a> ) </small>
	</div>
</div>



<!-- PRCV -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/PRCV24.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Bidirectional Alternating Fusion Network for RGB-T Salient Object Detection </b></font><br />
		<small> Zhengzheng Tu, Danying Lin, Bo Jiang, Le Gu, <b>Kunpeng Wang</b>, Sulan Zhai </small><br />
		<small> Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2024 </small><br />
		<!-- <small> [TL;DR] DiMR is a new diffusion backbone that achieves state-of-the-art image generation. For example, on the ImageNet 256 x 256 benchmark, DiMR with only 505M parameters surpasses all existing image generation models of various sizes, without any bells and whistles.  </small><br /> -->
		<small>( <a href="https://link.springer.com/chapter/10.1007/978-981-97-8685-5_3">Paper</a> | <a href="https://link.springer.com/chapter/10.1007/978-981-97-8685-5_3">Code</a> ) </small>
	</div>
</div>



<!-- EAAI23 -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/EAAI23.png"/>
	</div>
	<div class="intro">
		<font size=4><b> Multimodal salient object detection via adversarial learning with collaborative generator </b></font><br />
		<small> Zhengzheng Tu, Wenfang Yang, <b>Kunpeng Wang</b>, Amir Hussain, Bin Luo, Chenglong Li </small><br />
		<small> Engineering Applications of Artificial Intelligence (EAAI), 2023 </small><br />
		<!-- <small> [TL;DR] DiMR is a new diffusion backbone that achieves state-of-the-art image generation. For example, on the ImageNet 256 x 256 benchmark, DiMR with only 505M parameters surpasses all existing image generation models of various sizes, without any bells and whistles.  </small><br /> -->
		<small>( <a href="https://napier-repository.worktribe.com/preview/3030989/Multimodal_Salient_Object_Detection_via_Adversarial_Learning_with_Collaborative_Generator.pdf">Paper</a> | <a href="https://napier-repository.worktribe.com/preview/3030989/Multimodal_Salient_Object_Detection_via_Adversarial_Learning_with_Collaborative_Generator.pdf">Code</a> ) </small>
	</div>
</div>



<!-- <b> Other Papers: </b>
<div class="project">
	<div class="teaser">
		<img src="/imgs/UDA.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles</b></font><br />
		<small> Qing Liu, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo, <b>Qihao Liu</b>, Xiaoding Yuan, Jiteng Mu, Weichao Qiu, Alan Yuille </small><br />
		<small> Computer Vision and Pattern Recognition Conference (CVPR), Oral, 2022 </small><br />
		<small>( <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Part_Segmentation_Through_Unsupervised_Domain_Adaptation_From_Synthetic_Vehicles_CVPR_2022_paper.pdf">PDF</a> | <a href="https://qliu24.github.io/udapart/">dataset</a> )</small>
	</div>
</div> -->


<!-- <zero-md src="2009-05-15-edge-case-nested-and-mixed-lists.md"></zero-md> -->


<!-- <ul>
	{% for post in site.posts %}
		<li>
			<a href="{{ post.url 2009-05-15-edge-case-nested-and-mixed-lists}}">{{ Edge Case: Nested and Mixed Lists }}</a>
		</li>
	{% endfor %}
</ul> -->



<h2> Projects </h2>
<hr />
<ul style="margin:0px auto">
	<li style="margin:0px auto"><small>[2025.1-2026.12] Youth Talents Support Project - Doctoral Student Special Program, <strong><em>Host, 40,000 RMB</em></strong></small></li>
	<li style="margin:0px auto"><small>[2025.3-2025.12] Anhui Province New Era Parenting Quality Project, <strong><em>Host, 30,000 RMB</em></strong></small></li>
	<li style="margin:0px auto"><small>[2023.12-2025.3] Outstanding Doctoral Dissertation Development Program of Anhui University, <strong><em>Host, 40,000 RMB</em></strong></small></li>
	<li style="margin:0px auto"><small>[2021.12-2023.4] Outstanding Master's Dissertation Development Program of Anhui University , <strong><em>Host, 30,000 RMB</em></strong></small></li>
</ul>



<h2> Selected Awards and Honors </h2>
<hr />
<ul style="margin:0px auto">
	<li style="margin:0px auto"><small>[2024.05] China Scholarship Council (CSC), <strong><em>Funded</em></strong></small></li>
	<li style="margin:0px auto"><small>[2021.12] Anhui Key Laboratory of Multimodal Cognition, <strong><em>Academic Rising Star</em></strong></small></li>
	<li style="margin:0px auto"><small>[2020.07] China University Computer Competition WeChat Mini Program Application Development Competition, <strong><em>First Prize in East China Division</em></strong></small></li>
	<!-- <li style="margin:0px auto"><small>[2019.11] National Inspirational Scholarship, <strong><em>Funded</em></strong></small></li>
	<li style="margin:0px auto"><small>[2018.11] National Inspirational Scholarship, <strong><em>Funded</em></strong></small></li> -->
	<li style="margin:0px auto"><small>[2018.11] National College Student Software Testing Competition Finals Embedded Individual Competition, <strong><em>National Third Prize</em></strong></small></li>
</ul>

</body>
